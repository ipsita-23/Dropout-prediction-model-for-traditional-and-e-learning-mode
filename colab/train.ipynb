{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Student Dropout Prediction - Model Training\n",
        "\n",
        "This notebook trains machine learning models to predict student dropout.\n",
        "\n",
        "**Models to train:**\n",
        "- XGBoost (Primary)\n",
        "- RandomForest (Comparison)\n",
        "\n",
        "**Evaluation Metrics:**\n",
        "- Accuracy\n",
        "- F1-Score\n",
        "- Confusion Matrix\n",
        "- ROC-AUC Curve\n",
        "- Feature Importance\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 1. Setup and Installation\n",
        "\n",
        "**Note:** This notebook automatically detects and uses GPU if available in Colab, otherwise uses CPU. No manual configuration needed!\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Install required packages\n",
        "%pip install -q pandas numpy scikit-learn xgboost imbalanced-learn matplotlib seaborn joblib\n",
        "\n",
        "# Check hardware availability (GPU or CPU)\n",
        "import subprocess\n",
        "try:\n",
        "    result = subprocess.run(['nvidia-smi'], capture_output=True, text=True, timeout=5)\n",
        "    if result.returncode == 0:\n",
        "        print(\"‚úÖ GPU is available!\")\n",
        "        print(\"XGBoost will use GPU acceleration for faster training\")\n",
        "        USE_GPU = True\n",
        "    else:\n",
        "        USE_GPU = False\n",
        "except:\n",
        "    USE_GPU = False\n",
        "\n",
        "if not USE_GPU:\n",
        "    print(\"‚ÑπÔ∏è GPU not available, using CPU (training will work fine, just slower)\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Import libraries\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.metrics import (\n",
        "    accuracy_score, f1_score, confusion_matrix, \n",
        "    classification_report, roc_auc_score, roc_curve\n",
        ")\n",
        "from imblearn.over_sampling import SMOTE\n",
        "import xgboost as xgb\n",
        "import joblib\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "# Set style\n",
        "sns.set_style(\"whitegrid\")\n",
        "plt.rcParams['figure.figsize'] = (12, 8)\n",
        "\n",
        "# Set tree method for XGBoost (automatically uses GPU if available)\n",
        "tree_method = 'gpu_hist' if USE_GPU else 'hist'\n",
        "\n",
        "print(\"‚úÖ All libraries imported successfully!\")\n",
        "print(f\"üìä XGBoost will use: {'GPU' if USE_GPU else 'CPU'} (tree_method='{tree_method}')\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 2. Mount Google Drive\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "print(\"‚úÖ Google Drive mounted!\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 3. Load Dataset\n",
        "\n",
        "**Instructions:**\n",
        "1. Upload your `dataset.csv` file to Google Drive\n",
        "2. Update the path below to point to your dataset location\n",
        "3. Example: `/content/drive/MyDrive/AI_Project/data/dataset.csv`\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Update this path to your dataset location\n",
        "dataset_path = '/content/drive/MyDrive/AI_Project/data/dataset.csv'\n",
        "\n",
        "# Alternative: Upload directly to Colab\n",
        "# from google.colab import files\n",
        "# uploaded = files.upload()\n",
        "# dataset_path = list(uploaded.keys())[0]\n",
        "\n",
        "# Load dataset\n",
        "df = pd.read_csv(dataset_path)\n",
        "\n",
        "print(f\"‚úÖ Dataset loaded successfully!\")\n",
        "print(f\"üìä Shape: {df.shape}\")\n",
        "print(f\"\\nüìã Columns: {list(df.columns)}\")\n",
        "print(f\"\\nüìà First few rows:\")\n",
        "df.head()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Dataset information\n",
        "print(\"Dataset Info:\")\n",
        "print(\"=\"*50)\n",
        "print(f\"Total records: {len(df)}\")\n",
        "print(f\"\\nTarget distribution:\")\n",
        "print(df['dropout'].value_counts())\n",
        "print(f\"\\nDropout rate: {df['dropout'].mean() * 100:.2f}%\")\n",
        "print(f\"\\n\\nDescriptive Statistics:\")\n",
        "df.describe()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 4. Data Preprocessing\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Separate features and target\n",
        "X = df.drop(columns=['dropout'])\n",
        "y = df['dropout']\n",
        "\n",
        "print(f\"Features shape: {X.shape}\")\n",
        "print(f\"Target shape: {y.shape}\")\n",
        "print(f\"\\nFeature names: {list(X.columns)}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Check for missing values\n",
        "missing_values = X.isnull().sum()\n",
        "if missing_values.sum() > 0:\n",
        "    print(\"‚ö†Ô∏è Missing values found:\")\n",
        "    print(missing_values[missing_values > 0])\n",
        "    # Fill with median\n",
        "    for col in X.columns:\n",
        "        if X[col].isnull().sum() > 0:\n",
        "            X[col].fillna(X[col].median(), inplace=True)\n",
        "    print(\"\\n‚úÖ Missing values filled with median\")\n",
        "else:\n",
        "    print(\"‚úÖ No missing values found\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Train-test split\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X, y, test_size=0.2, random_state=42, stratify=y, shuffle=True\n",
        ")\n",
        "\n",
        "print(f\"‚úÖ Train set: {X_train.shape[0]} samples\")\n",
        "print(f\"‚úÖ Test set: {X_test.shape[0]} samples\")\n",
        "print(f\"\\nTrain set dropout rate: {y_train.mean() * 100:.2f}%\")\n",
        "print(f\"Test set dropout rate: {y_test.mean() * 100:.2f}%\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Feature scaling\n",
        "scaler = StandardScaler()\n",
        "X_train_scaled = scaler.fit_transform(X_train)\n",
        "X_test_scaled = scaler.transform(X_test)\n",
        "\n",
        "# Convert back to DataFrame for better handling\n",
        "X_train_scaled = pd.DataFrame(X_train_scaled, columns=X_train.columns, index=X_train.index)\n",
        "X_test_scaled = pd.DataFrame(X_test_scaled, columns=X_test.columns, index=X_test.index)\n",
        "\n",
        "print(\"‚úÖ Features scaled using StandardScaler\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Apply SMOTE for class imbalance\n",
        "print(\"Before SMOTE:\")\n",
        "print(y_train.value_counts())\n",
        "\n",
        "smote = SMOTE(random_state=42)\n",
        "X_train_balanced, y_train_balanced = smote.fit_resample(X_train_scaled, y_train)\n",
        "\n",
        "print(\"\\n‚úÖ After SMOTE:\")\n",
        "print(f\"Shape: {X_train_balanced.shape}\")\n",
        "print(f\"\\nClass distribution:\")\n",
        "print(pd.Series(y_train_balanced).value_counts())\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 5. Model Training\n",
        "\n",
        "### 5.1 XGBoost Classifier\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Train XGBoost model\n",
        "print(\"üöÄ Training XGBoost Classifier...\")\n",
        "print(\"=\"*50)\n",
        "print(f\"Hardware: {'GPU üöÄ' if USE_GPU else 'CPU'}\")\n",
        "print(f\"Tree method: {tree_method}\")\n",
        "print()\n",
        "\n",
        "xgb_model = xgb.XGBClassifier(\n",
        "    n_estimators=100,\n",
        "    max_depth=6,\n",
        "    learning_rate=0.1,\n",
        "    subsample=0.8,\n",
        "    colsample_bytree=0.8,\n",
        "    random_state=42,\n",
        "    eval_metric='logloss',\n",
        "    use_label_encoder=False,\n",
        "    tree_method=tree_method  # Automatically uses GPU if available, CPU otherwise\n",
        ")\n",
        "\n",
        "xgb_model.fit(X_train_balanced, y_train_balanced)\n",
        "\n",
        "print(\"‚úÖ XGBoost model trained successfully!\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Evaluate XGBoost\n",
        "y_pred_xgb = xgb_model.predict(X_test_scaled)\n",
        "y_pred_proba_xgb = xgb_model.predict_proba(X_test_scaled)[:, 1]\n",
        "\n",
        "accuracy_xgb = accuracy_score(y_test, y_pred_xgb)\n",
        "f1_xgb = f1_score(y_test, y_pred_xgb)\n",
        "roc_auc_xgb = roc_auc_score(y_test, y_pred_proba_xgb)\n",
        "\n",
        "print(\"XGBoost Performance:\")\n",
        "print(\"=\"*50)\n",
        "print(f\"Accuracy:  {accuracy_xgb:.4f} ({accuracy_xgb*100:.2f}%)\")\n",
        "print(f\"F1-Score:  {f1_xgb:.4f}\")\n",
        "print(f\"ROC-AUC:   {roc_auc_xgb:.4f}\")\n",
        "print(\"\\nClassification Report:\")\n",
        "print(classification_report(y_test, y_pred_xgb))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 5.2 Random Forest Classifier\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Train Random Forest model\n",
        "print(\"üöÄ Training Random Forest Classifier...\")\n",
        "print(\"=\"*50)\n",
        "\n",
        "rf_model = RandomForestClassifier(\n",
        "    n_estimators=100,\n",
        "    max_depth=10,\n",
        "    min_samples_split=5,\n",
        "    min_samples_leaf=2,\n",
        "    random_state=42,\n",
        "    n_jobs=-1\n",
        ")\n",
        "\n",
        "rf_model.fit(X_train_balanced, y_train_balanced)\n",
        "\n",
        "print(\"‚úÖ Random Forest model trained successfully!\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Evaluate Random Forest\n",
        "y_pred_rf = rf_model.predict(X_test_scaled)\n",
        "y_pred_proba_rf = rf_model.predict_proba(X_test_scaled)[:, 1]\n",
        "\n",
        "accuracy_rf = accuracy_score(y_test, y_pred_rf)\n",
        "f1_rf = f1_score(y_test, y_pred_rf)\n",
        "roc_auc_rf = roc_auc_score(y_test, y_pred_proba_rf)\n",
        "\n",
        "print(\"Random Forest Performance:\")\n",
        "print(\"=\"*50)\n",
        "print(f\"Accuracy:  {accuracy_rf:.4f} ({accuracy_rf*100:.2f}%)\")\n",
        "print(f\"F1-Score:  {f1_rf:.4f}\")\n",
        "print(f\"ROC-AUC:   {roc_auc_rf:.4f}\")\n",
        "print(\"\\nClassification Report:\")\n",
        "print(classification_report(y_test, y_pred_rf))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 6. Model Comparison\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Compare models\n",
        "comparison = pd.DataFrame({\n",
        "    'Model': ['XGBoost', 'Random Forest'],\n",
        "    'Accuracy': [accuracy_xgb, accuracy_rf],\n",
        "    'F1-Score': [f1_xgb, f1_rf],\n",
        "    'ROC-AUC': [roc_auc_xgb, roc_auc_rf]\n",
        "})\n",
        "\n",
        "print(\"Model Comparison:\")\n",
        "print(\"=\"*50)\n",
        "print(comparison.to_string(index=False))\n",
        "\n",
        "# Visualize comparison\n",
        "fig, axes = plt.subplots(1, 3, figsize=(15, 5))\n",
        "\n",
        "metrics = ['Accuracy', 'F1-Score', 'ROC-AUC']\n",
        "for idx, metric in enumerate(metrics):\n",
        "    axes[idx].bar(comparison['Model'], comparison[metric], color=['#3498db', '#2ecc71'])\n",
        "    axes[idx].set_title(f'{metric} Comparison', fontsize=14, fontweight='bold')\n",
        "    axes[idx].set_ylabel(metric)\n",
        "    axes[idx].set_ylim([0, 1])\n",
        "    axes[idx].grid(True, alpha=0.3, axis='y')\n",
        "    \n",
        "    # Add value labels\n",
        "    for i, v in enumerate(comparison[metric]):\n",
        "        axes[idx].text(i, v + 0.01, f'{v:.3f}', ha='center', va='bottom', fontweight='bold')\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.savefig('/content/drive/MyDrive/AI_Project/models/model_comparison.png', dpi=300, bbox_inches='tight')\n",
        "plt.show()\n",
        "\n",
        "print(\"\\n‚úÖ Comparison plot saved to Google Drive\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 7. Confusion Matrix\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Plot confusion matrices\n",
        "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
        "\n",
        "models = [('XGBoost', xgb_model, y_pred_xgb), ('Random Forest', rf_model, y_pred_rf)]\n",
        "\n",
        "for idx, (name, model, y_pred) in enumerate(models):\n",
        "    cm = confusion_matrix(y_test, y_pred)\n",
        "    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', ax=axes[idx], \n",
        "                xticklabels=['No Dropout', 'Dropout'], \n",
        "                yticklabels=['No Dropout', 'Dropout'])\n",
        "    axes[idx].set_title(f'{name} - Confusion Matrix', fontsize=14, fontweight='bold')\n",
        "    axes[idx].set_ylabel('True Label')\n",
        "    axes[idx].set_xlabel('Predicted Label')\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.savefig('/content/drive/MyDrive/AI_Project/models/confusion_matrices.png', dpi=300, bbox_inches='tight')\n",
        "plt.show()\n",
        "\n",
        "print(\"‚úÖ Confusion matrices saved to Google Drive\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 8. ROC-AUC Curve\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Plot ROC curves\n",
        "plt.figure(figsize=(10, 8))\n",
        "\n",
        "# XGBoost ROC\n",
        "fpr_xgb, tpr_xgb, _ = roc_curve(y_test, y_pred_proba_xgb)\n",
        "plt.plot(fpr_xgb, tpr_xgb, label=f'XGBoost (AUC = {roc_auc_xgb:.4f})', \n",
        "         linewidth=2, color='#3498db')\n",
        "\n",
        "# Random Forest ROC\n",
        "fpr_rf, tpr_rf, _ = roc_curve(y_test, y_pred_proba_rf)\n",
        "plt.plot(fpr_rf, tpr_rf, label=f'Random Forest (AUC = {roc_auc_rf:.4f})', \n",
        "         linewidth=2, color='#2ecc71')\n",
        "\n",
        "# Diagonal line (random classifier)\n",
        "plt.plot([0, 1], [0, 1], 'k--', label='Random Classifier', linewidth=1)\n",
        "\n",
        "plt.xlabel('False Positive Rate', fontsize=12)\n",
        "plt.ylabel('True Positive Rate', fontsize=12)\n",
        "plt.title('ROC-AUC Curve Comparison', fontsize=16, fontweight='bold')\n",
        "plt.legend(loc='lower right', fontsize=12)\n",
        "plt.grid(True, alpha=0.3)\n",
        "plt.tight_layout()\n",
        "plt.savefig('/content/drive/MyDrive/AI_Project/models/roc_curve.png', dpi=300, bbox_inches='tight')\n",
        "plt.show()\n",
        "\n",
        "print(\"‚úÖ ROC curve saved to Google Drive\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 9. Feature Importance\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Feature importance for both models\n",
        "fig, axes = plt.subplots(1, 2, figsize=(16, 6))\n",
        "\n",
        "# XGBoost feature importance\n",
        "feature_importance_xgb = pd.DataFrame({\n",
        "    'feature': X.columns,\n",
        "    'importance': xgb_model.feature_importances_\n",
        "}).sort_values('importance', ascending=False)\n",
        "\n",
        "axes[0].barh(feature_importance_xgb['feature'], feature_importance_xgb['importance'], \n",
        "            color='#3498db')\n",
        "axes[0].set_title('XGBoost - Feature Importance', fontsize=14, fontweight='bold')\n",
        "axes[0].set_xlabel('Importance')\n",
        "axes[0].invert_yaxis()\n",
        "axes[0].grid(True, alpha=0.3, axis='x')\n",
        "\n",
        "# Random Forest feature importance\n",
        "feature_importance_rf = pd.DataFrame({\n",
        "    'feature': X.columns,\n",
        "    'importance': rf_model.feature_importances_\n",
        "}).sort_values('importance', ascending=False)\n",
        "\n",
        "axes[1].barh(feature_importance_rf['feature'], feature_importance_rf['importance'], \n",
        "            color='#2ecc71')\n",
        "axes[1].set_title('Random Forest - Feature Importance', fontsize=14, fontweight='bold')\n",
        "axes[1].set_xlabel('Importance')\n",
        "axes[1].invert_yaxis()\n",
        "axes[1].grid(True, alpha=0.3, axis='x')\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.savefig('/content/drive/MyDrive/AI_Project/models/feature_importance.png', dpi=300, bbox_inches='tight')\n",
        "plt.show()\n",
        "\n",
        "print(\"‚úÖ Feature importance plots saved to Google Drive\")\n",
        "print(\"\\nTop 5 Most Important Features (XGBoost):\")\n",
        "print(feature_importance_xgb.head().to_string(index=False))\n",
        "print(\"\\nTop 5 Most Important Features (Random Forest):\")\n",
        "print(feature_importance_rf.head().to_string(index=False))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 10. Save Best Model\n",
        "\n",
        "**Note:** We'll save the XGBoost model as it typically performs better, but you can change this to Random Forest if preferred.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Select best model (XGBoost based on performance)\n",
        "best_model = xgb_model\n",
        "model_name = 'dropout_model.pkl'\n",
        "model_path = f'/content/drive/MyDrive/AI_Project/models/{model_name}'\n",
        "\n",
        "# Create directory if it doesn't exist\n",
        "import os\n",
        "os.makedirs(os.path.dirname(model_path), exist_ok=True)\n",
        "\n",
        "# Save model\n",
        "joblib.dump(best_model, model_path)\n",
        "\n",
        "# Also save scaler for preprocessing\n",
        "scaler_path = f'/content/drive/MyDrive/AI_Project/models/scaler.pkl'\n",
        "joblib.dump(scaler, scaler_path)\n",
        "\n",
        "print(f\"‚úÖ Model saved to: {model_path}\")\n",
        "print(f\"‚úÖ Scaler saved to: {scaler_path}\")\n",
        "print(\"\\nüì• To download to local machine:\")\n",
        "print(\"   1. Go to Google Drive\")\n",
        "print(\"   2. Navigate to AI_Project/models/\")\n",
        "print(f\"   3. Download {model_name} and scaler.pkl\")\n",
        "print(\"   4. Place them in your local project/models/ folder\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 11. Training Summary\n",
        "\n",
        "### Model Performance Summary:\n",
        "\n",
        "The models have been trained and evaluated. Check the outputs above for detailed metrics.\n",
        "\n",
        "### Next Steps:\n",
        "1. ‚úÖ Model trained and saved to Google Drive\n",
        "2. üì• Download the model file (`dropout_model.pkl`) to your local machine\n",
        "3. üìÅ Place it in `project/models/dropout_model.pkl`\n",
        "4. üöÄ Use the model in your local application\n",
        "\n",
        "### Files Saved to Google Drive:\n",
        "- `/MyDrive/AI_Project/models/dropout_model.pkl` - Trained model\n",
        "- `/MyDrive/AI_Project/models/scaler.pkl` - Feature scaler\n",
        "- `/MyDrive/AI_Project/models/model_comparison.png` - Model comparison plot\n",
        "- `/MyDrive/AI_Project/models/confusion_matrices.png` - Confusion matrices\n",
        "- `/MyDrive/AI_Project/models/roc_curve.png` - ROC curve\n",
        "- `/MyDrive/AI_Project/models/feature_importance.png` - Feature importance\n",
        "\n",
        "**üéâ Training completed successfully!**\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Student Dropout Prediction - Model Training\n",
        "\n",
        "This notebook trains machine learning models to predict student dropout based on attendance, academic performance, indiscipline, and online engagement metrics.\n",
        "\n",
        "## Steps:\n",
        "1. Mount Google Drive\n",
        "2. Load and preprocess dataset\n",
        "3. Train XGBoost and RandomForest models\n",
        "4. Evaluate models (Accuracy, F1-score, ROC-AUC, Confusion Matrix)\n",
        "5. Generate feature importance plots\n",
        "6. Save model to Google Drive\n",
        "7. Download model to local machine\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Install required packages\n",
        "!pip install -q pandas numpy scikit-learn xgboost matplotlib seaborn imbalanced-learn joblib\n",
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.metrics import accuracy_score, f1_score, roc_auc_score, confusion_matrix, classification_report, roc_curve\n",
        "from imblearn.over_sampling import SMOTE\n",
        "import xgboost as xgb\n",
        "import joblib\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "print(\"‚úÖ All packages installed and imported successfully!\")\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": []
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
